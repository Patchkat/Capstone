---
layout: post
title: Results
date: 2020-05-06 12:30:00
author: Jono Jenkens
---

## After all of this work and 24 hours of training, it was finally time to take the model out into the world. 
<center>
    <img src="{{site.baseurl}}/assets/photos/training.png?raw=true" alt="Data about the training of the neural network">
</center>

***
# Training Results
As you can see above, the model claimed to have an accuracy of 99% by the end of training.  That seems insane for a project thrown together by a random high school student with very little experience surrounding AI, which is why it seems likely that the model overfit. As you can see below, there were a few different classes that the network often messed up when deciding what was going on in an image. 
<center>
    <img src="{{site.baseurl}}/assets/photos/labels.png?raw=true" alt="Images labeled with what they actually were vs. what the network predicted">
</center>
Above each image is a few values, specifically what the network predicted the image was, what the actual image was, the amount of loss from that, and then how certain the network was about the correct class, rather than what it actually chose.  Again below, you can see a confusion matrix.  This is a simple table, with the rows and columns being all the different class, and then each square represents one set of correct vs. predicted outputs.
<center>
    <img src="{{site.baseurl}}/assets/photos/confusion.png?raw=true" alt="Confusion matrix displaying what classes the network mixed up">
</center>
As you can see, the network almost always predicted the correct class. However, there are a few instances where the model appears to have gotten confused and chosen a different class from what the image was depicting. The only time that it got confused twice was between the classes for talking to passengers and going makeup.  That is certainly an interesting combination, but it does make sense as all of the other classes are either not distracted or something related to phones/car systems, so the last two would be most easily confused.  Luckily, the network never seems to have gotten a false positive for paying attention, so that means that for my specific purposes the network is good.  I don’t specifically care why the driver is distracted; I just want to know if they are. 

The only way to truly test whether the model works is to give it brand new data, specifically using myself as the subject. In order to do that, I needed to get a camera mounted in my car and connected to a computer with some code to feed the images into the network.  Luckily, near the beginning of my capstone I had gotten the Microsoft Azure Kinect DK2 from campus for that specific purpose. 

***
# Real World 
Now, the first issue I ran into was saving the network after all of this time that had gone into training it. Honestly, I am still not entirely sure what the issue was that caused the network to be unable to save. I really needed to save the network, as if I didn’t then all of the time spend training would have been a waste. In the end, I found that I could export the network to a pickle file, a serialized version of the network.  Because it was serialized, the network was just a string of characters saved to a file.  Luckily, fast.ai has a way to then take that file, decode it, and get your neural network back up and running. 

So, I took my pickle file, reimported it to a new jupyter notebook file, and got to work.  This work involved getting the camera setup in my car somehow to stay at about the same spot as the State Farm dataset photos were taken.  To do this, I finally settled on having my parent stand outside the car with the window open. This obviously meant that the car wasn’t moving, but I hoped that would not affect the ability of the network to detect whether or not I was paying attention.  Plus, creating a new dataset while driving around distracted would go against everything I said in my first two blog posts about the dangers of distracted driving. 

To capture the images I used a python OpenCV wrapper. OpenCV is a really great library focused around computer vision and general camera stuff. Perfect for my need, as I needed to programmatically take in a camera stream and then save images from that.  In order to do this, I created the simple program shown below. 

***
```python
vid = cv2.VideoCapture(1) 
img = 0 
while(True):  
    ret, frame = vid.read()  
    cv2.imshow('frame', frame) 
    if random.randrange(20) == 3: 
        cv2.imwrite("C:/Users/Jono_Jenkens/Documents/tests/images/{}.jpg".format(img), frame) 
        img += 1 
    if cv2.waitKey(1) & 0xFF == ord('q'):  
        break 
     
vid.release()   
cv2.destroyAllWindows() 
``` 
***
In this program, I take a camera stream from the Azure Kinect DK2, then randomly generate an int and check if it is 3.  If it is, then it save that frame as a jpg. If it is not, then it just loops around again.  This continues until I close the application, allowing me to get a lot of images quite quickly. 

Now, after setting up in my car and getting all the images, I was ready to put them into the network to get it to predict whether or not I was distracted.  It was the final piece of work for my capstone, what I had been working towards this whole time. 

***
# Predictions
So, I took my images, got rid of any where I was transitioning from say talking on the phone to texting, and then set the network to work.  I went through every image, displayed it, and then had the network decide what it thought each image was showing.  It took quite a few minutes to get through all of them on my tablet, but luckily not super long, as the network itself wasn’t being trained, it was just predicting each image. I did run into some trouble with apparently too many images being opened, and it didn’t suggest any way to close the images, so I just reduced the amount of images that I was testing. 

As the results came back from the predictions, I was initially surprised.  The network had actually worked!  It detected that in the first image I was paying attention to the road.  However, shortly after it started to go wrong again.  The network just said that the other times I was paying attention I was actually distracted.  I don’t know if it is was the camera angle, or something else, but it was dead set on the fact that I was not paying attention. 
<center>
    <img src="{{site.baseurl}}/assets/photos/predictions.png?raw=true" alt="Predictions from the network as to whether I am paying attention">
</center>
Most concerning to me was the fact that when I was “texting”, the network decided that I was actually not distracted.  It did correctly decide that I was distracted while talking on the phone, but I suspect that may have partially been luck, as opposed to actually being the network correctly seeing that I was distracted. 

In the end, it seemed the perhaps I could not as easily just transfer the learning from the state farm dataset to myself.  Many factors likely contributed to the network being unable to properly detect my state of distraction, including a different angle and a much better camera quality.  It seems that overall, this probably needs a bit more work to get it to correctly detect whether or not a driver is truly paying attention. 
